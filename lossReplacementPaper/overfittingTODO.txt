I think the biggest thing to glean from this is the whole overfitting philosophy
- phi can stabilize even as the model gets better so using that to determine when the training loop
  should stop is probably pretty aspirable!

THINKING THAT LOG-H IS THE MOVEEEE WOOOOO - on second thought it may be the value that the
heatmap brings which is REALLY interesting - needs more thought for sure so running the tests

noHWeird shows how the phi gets bad with overfitting which is really important! I want to start
saving phi-values, I think noticing those trends will also be really important.

OOPSIES FIGURED OUT SILLY BUSINESS SO FIXED THAT LMAO - starting a rerun and hopefully less
epochs gets us more data faster!

Need to make placebo script after running all the new tests! (*** HAVENT RAN THIS YET LOL, I TABLED IT FOR LATER)

Proof pickles:
- log-H0
- log-H1
- log-H2
- linear-H0
- linear-P2

Disproof pickles:
- no-HWeird
- no-P
- linear-P0
- linear-P1
- linear-P3
- linear-P4
- log-P